llm_providers:
  - name: my_bam
    type: bam
    url: "https://bam-api.res.ibm.com"
    credentials_path: bam_api_key.txt
    models:
      - name: ibm/granite-13b-chat-v2
        context_window_size: 2000
        response_token_limit: 500
  - name: my_openai
    type: openai
    url: "https://api.openai.com/v1"
    credentials_path: openai_api_key.txt
    models:
      - name: gpt-4-1106-preview
      - name: gpt-3.5-turbo
  - name: my_azure_openai
    type: azure_openai
    url: "https://myendpoint.openai.azure.com/"
    credentials_path: azure_openai_api_key.txt
    deployment_name: my_azure_openai_deployment_name
    models:
      - name: gpt-3.5-turbo
  - name: my_watsonx
    type: watsonx
    url: "https://us-south.ml.cloud.ibm.com"
    credentials_path: watsonx_api_key.txt
    project_id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX
    models:
      - name: ibm/granite-13b-chat-v2
ols_config:
  reference_content:
#    product_docs_index_path: "./vector-db/ocp-product-docs"
#    product_docs_index_id: product
#    embeddings_model_path: "./embeddings_model"
  conversation_cache:
    type: memory
    memory:
      max_entries: 1000
  logging_config:
    app_log_level: info
    lib_log_level: warning
  default_provider: my_bam
  default_model: ibm/granite-13b-chat-v2
  # query_filters:
  #   - name: foo_filter
  #     pattern: '\b(?:foo)\b'
  #     replace_with: "deployment"
  #   - name: bar_filter
  #     pattern: '\b(?:bar)\b'
  #     replace_with: "openshift"
  authentication_config:
    k8s_cluster_api: "https://api.example.com:6443"
    k8s_ca_cert_path: "/Users/home/ca.crt"
    skip_tls_verification: false
dev_config:
  enable_dev_ui: true
  # llm_params:
  #   temperature_override: 0
  # disable_question_validation: false
  # disable_auth: false
  # k8s_auth_token: optional_token_when_no_available_kube_config
